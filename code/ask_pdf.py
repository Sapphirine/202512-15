# ask_pdf.py (å¢å¼ºç‰ˆ - æ˜¾ç¤ºå®Œæ•´ output)

import numpy as np
import faiss
import pickle
import requests
from sentence_transformers import SentenceTransformer
import os

# === é…ç½® ===
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"  # ä¸­æ–‡å»ºè®®æ¢ "BAAI/bge-small-zh-v1.5"
VLLM_URL = "http://localhost:8000/v1/chat/completions"
RAG_DATA_DIR = "rag_data"

# === åŠ è½½å‘é‡åº“å’Œå…ƒæ•°æ® ===
print("ğŸ” æ­£åœ¨åŠ è½½å‘é‡åº“å’Œå…ƒæ•°æ®...")
model_emb = SentenceTransformer(EMBEDDING_MODEL_NAME)
index = faiss.read_index(os.path.join(RAG_DATA_DIR, "faiss.index"))
with open(os.path.join(RAG_DATA_DIR, "texts.pkl"), "rb") as f:
    texts = pickle.load(f)
with open(os.path.join(RAG_DATA_DIR, "metadatas.pkl"), "rb") as f:
    metadatas = pickle.load(f)

def retrieve_with_metadata(query: str, top_k: int = 3):
    query_vec = model_emb.encode([query])
    D, I = index.search(np.array(query_vec, dtype=np.float32), top_k)
    results = []
    for idx in I[0]:
        results.append({
            "text": texts[idx],
            "source": metadatas[idx]["source"],
            "page": metadatas[idx]["page"]
        })
    return results

def ask_pdf(question: str):
    print("\n" + "="*60)
    print(f"â“ é—®é¢˜: {question}")
    print("="*60)

    # 1. æ£€ç´¢ç›¸å…³æ®µè½
    retrieved = retrieve_with_metadata(question, top_k=3)
    context_text = "\n\n".join([r["text"] for r in retrieved])

    print("ğŸ“„ æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆæ¥è‡ª PDFï¼‰:")
    print("-" * 40)
    for i, r in enumerate(retrieved, 1):
        print(f"[{i}] æ¥æº: {r['source']} (ç¬¬ {r['page']} é¡µ)")
        print(f"    å†…å®¹: {r['text'][:200]}...\n")

    # 2. æ„é€  prompt
    messages = [
        {
            "role": "system",
            "content": (
                "You are a helpful assistant that answers questions based ONLY on the provided document context. "
                "Cite the source if possible. If the answer is not in the context, say 'I don't know based on the provided documents.'"
            )
        },
        {
            "role": "user",
            "content": f"Document context:\n{context_text}\n\nQuestion: {question}"
        }
    ]

    # 3. è°ƒç”¨ vLLM
    print("ğŸ§  æ­£åœ¨è°ƒç”¨ Qwen2.5-3B-Instruct...")
    try:
        response = requests.post(
            VLLM_URL,
            json={
                "model": "Qwen/Qwen2.5-3B-Instruct",
                "messages": messages,
                "max_tokens": 512,
                "temperature": 0.1
            },
            timeout=120
        )
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ è°ƒç”¨ vLLM å¤±è´¥: {e}")
        return

    result = response.json()
    answer = result["choices"][0]["message"]["content"].strip()

    # 4. æ˜¾ç¤ºå®Œæ•´è¾“å‡º
    print("âœ… æ¨¡å‹è¾“å‡º:")
    print("-" * 40)
    print(answer)
    print("-" * 40)

    # 5. æ˜¾ç¤ºæ¥æº
    sources = list(set([f"{r['source']} (p.{r['page']})" for r in retrieved]))
    print(f"ğŸ“š æ¥æº: {', '.join(sources)}")
    print("="*60 + "\n")

# === äº¤äº’å¼é—®ç­” ===
if __name__ == "__main__":
    print("ğŸ¤– PDF RAG é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡ºã€‚")
    while True:
        question = input("â“ è¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
        if question.lower() in ["quit", "exit", "q"]:
            print("ğŸ‘‹ å†è§ï¼")
            break
        if not question:
            continue
        ask_pdf(question)